# ===========================  Concise Model Explanation with Key Details ============================================
# ====================================================================================================================
# 1. Libraries Used and Purposes:
        # 1.1. Numpy and Pandas: Data handling and numerical operations.
        # 1.2. Scikit-learn: Data splitting, feature scaling (StandardScaler), class weights, and evaluation metrics.
        # 1.3. Scipy: Loading matrix files for gene expression data.
        # 1.4. TensorFlow/Keras: Neural network modeling (Sequential API, Dense layers, Dropout, Adam optimizer).
        # 1.5. Matplotlib and Seaborn: Visualization of model performance, including training history and confusion matrix.
        # 1.6. SHAP: Interpreting feature contributions and creating summary plots.
        
# 2. Data Preparation:
        # - Standardization: Scaled the gene expression data to ensure consistent ranges across features.
        # - Class Weights: Computed class weights to handle the class imbalance problem in PR, SD, and PD predictions.

# 3. Model Architecture:
        # - Neural Network Structure:
        # - Three fully connected hidden layers with ReLU activation to capture complex relationships between features.
        # - Dropout regularization (50%) applied after each hidden layer to prevent overfitting.
        # - Softmax output layer for multi-class classification across PR, SD, and PD.
        
# 4. Model Training:
        # - Class Weights Application: Applied class weights during training to ensure balanced learning.
        # - Optimizer and Loss Function: Adam optimizer with categorical crossentropy loss for efficient learning.

# 5. Evaluation Metrics:
        # - ROC-AUC Score: Evaluated the modelâ€™s performance using the multi-class ROC-AUC metric.
        # - Confusion Matrix and Classification Report: Provided comprehensive performance insights for each response class.

# 6. Model Interpretation with SHAP:
        # - SHAP DeepExplainer: Explained feature contributions using a random subset of training data as background.
        # - SHAP Summary Plots:
        # - Beeswarm plots showed individual sample impacts.
        # - Bar plots identified overall feature importance.


# ================================================ 1. LIBRARY IMPORTS ================================================
# ====================================================================================================================
# Core libraries for data handling and numerical operations
import numpy as np
import pandas as pd

# Machine learning preprocessing
from sklearn.model_selection import train_test_split  # For splitting the data
# For standardizing the features (MinMaxScaler, RobustScaler: KAVEH)
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler  

# Library for reading matrix files
from scipy.io import mmread  # To read .mtx files commonly used in genomics

# Neural network libraries from Keras
from tensorflow.keras.models import Sequential  # For creating a linear stack of layers in the neural network
from tensorflow.keras.layers import Dense, Dropout  # 'Dense' for fully connected layers, 'Dropout' for regularization
from tensorflow.keras.optimizers import Adam  # Optimizer for compiling the neural network

# Additional libraries for reproducibility across multiple runs
import tensorflow as tf
import random
import os

# =========================================== MODEL EVALUATION IMPORTS ===============================================
# Import libraries necessary for model evaluation after training
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score  # Evaluation metrics
import seaborn as sns  # For high-level, attractive statistical visualizations
import matplotlib.pyplot as plt  # For creating static, interactive, and animated visualizations in Python



# =================================================== 2. SET RANDOM SEED =============================================
# ====================================================================================================================
# Set a seed value
seed_value = 42  # we can choose any number

# 1. Set `PYTHONHASHSEED` environment variable at a fixed value
os.environ['PYTHONHASHSEED'] = str(seed_value)

# 2. Set `python` built-in pseudo-random generator at a fixed value
random.seed(seed_value)

# 3. Set `numpy` pseudo-random generator at a fixed value
np.random.seed(seed_value)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
tf.random.set_seed(seed_value)

# =========================================== why need to SET RANDOM SEED ============================================ 
# Setting a random seed in our code helps to ensure that the results are reproducible by making 
# the random number generation predictable. 
# In this pipeline, working with Keras and other libraries like NumPy and potentially TensorFlow, 
# we should set seeds for all these libraries to ensure consistency.


# ==================================================== 3. Loading data  ==============================================
# ====================================================================================================================

# Load gene names
gene_names_path = '/mnt/data/10.DL_scRNAseq_OMID_SURAJ/3.Study_Files/Subset_Seurat_Object/\
ICB_Only_NoICB_ICBNE_Excluded/Matrix_Files/ICB_Only_NoICB_ICBNE_Excluded_genes.tsv'
gene_names = pd.read_csv(gene_names_path, header=None, sep='\t')
gene_names_list = gene_names[0].tolist()  # Convert gene names to a list

# Load the expression matrix  
expression_matrix_path = '/mnt/data/10.DL_scRNAseq_OMID_SURAJ/3.Study_Files/\
Subset_Seurat_Object/ICB_Only_NoICB_ICBNE_Excluded/Matrix_Files/ICB_Only_NoICB_ICBNE_Excluded_expression_matrix.mtx'
expression_matrix = mmread(expression_matrix_path).toarray()

# Transpose the expression matrix so that genes become columns and cells become rows
X = pd.DataFrame(expression_matrix.T, columns=gene_names_list)

# Load your metadata
metadata_path = '/mnt/data/10.DL_scRNAseq_OMID_SURAJ/3.Study_Files/Subset_Seurat_Object/\
ICB_Only_NoICB_ICBNE_Excluded/ICB_Only_NoICB_ICBNE_Excluded_Cells_Metadata.csv'
metadata = pd.read_csv(metadata_path)

# One-hot encode the labels
y = pd.get_dummies(metadata['ICB_Response'])

# Ensure the gene names match the number of columns in the transposed expression matrix
if X.shape[1] != len(gene_names_list):
    raise ValueError("The number of gene names does not match the number of features in the transposed expression matrix.")


# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Print the number of genes and features to confirm correct loading and transposition
print(f"Number of genes in gene file: {len(gene_names_list)}")
print(f"Number of features (genes) in the matrix: {X.shape[1]}")



# Now your data is ready for model training

# ========================  4. Class Weights Calculation for my imbalance data (PR, SD, PD) ==========================
# ====================================================================================================================
from sklearn.utils.class_weight import compute_class_weight

# Calculate class weights
labels = metadata['ICB_Response'].values
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)
class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}


# =================================================== 4. BUILD MODEL =================================================
# ====================================================================================================================
# Define the model architecture


model = Sequential()

# Input layer: Implicitly defined by the input_dim parameter of the first Dense layer.

# First Dense layer with more units to capture more complex relationships
model.add(Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]))

# First Dropout layer remains the same
model.add(Dropout(0.5))

# Second Dense layer with 64 units, the same as the first to maintain capacity
model.add(Dense(64, activation='relu'))

# Second Dropout layer remains the same
model.add(Dropout(0.5))

# Adding a third Dense layer to add depth to the model
model.add(Dense(32, activation='relu'))

# Third Dropout layer remains the same
model.add(Dropout(0.5))

# Output layer with 3 units for the three classes (PR/SD/PD)
model.add(Dense(3, activation='softmax'))

# Compile the model with the Adam optimizer and categorical crossentropy loss
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])


 

# ================================================= 5. TRAIN MODEL ====================================================
# =====================================================================================================================
# Train the model
# "class_weight=class_weights_dict" was added here and this is the difference of this version compared 
history = model.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=8, class_weight=class_weights_dict)

# =================================================== 6. EVALUATE MODEL ==============================================
# ====================================================================================================================
# Evaluate on test set
evaluation = model.evaluate(X_test_scaled, y_test)
print(f'Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}')

# Plot training history
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 5))

# Add an overall title for the figure
plt.suptitle('ICB (No_ICB & ICB_NE Excluded)\Filters: (min.cells=3) + QC metrics (200<nFeature<6000 & \
                500<nCount<25000 & percent.mito<10) / \n batch_size=8'  )
# # (200<nFeature<6000 & 500<nCount<25000 & percent.mito<10)

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
 


plt.show()


	# ========================================== 7. DETAILED EVALUATION OF THE MODEL =====================================
# ====================================================================================================================
# Import necessary libraries for evaluation
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt


# Obtain the model's predictions on the test set
predictions = model.predict(X_test_scaled) 
predicted_classes = np.argmax(predictions, axis=1)  # Convert probabilities to class labels
actual_classes = np.argmax(y_test.to_numpy(), axis=1)  # Convert one-hot encoded labels back to class labels

# ROC-AUC Score
# Calculate the ROC-AUC score which provides an aggregate measure of performance across all possible classification thresholds
roc_auc = roc_auc_score(y_test, predictions, multi_class='ovr')
print(f'ROC-AUC Score: {roc_auc}')

# Classification Report
# Generate a classification report which includes precision, recall, f1-score for each class
print(classification_report(actual_classes, predicted_classes, target_names=y_test.columns))

# Confusion Matrix
# Generate and plot the confusion matrix to visualize the performance of the classification algorithm
conf_matrix = confusion_matrix(actual_classes, predicted_classes)
plt.figure(figsize=(10, 7))  # You can adjust the size as needed
sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=y_test.columns, yticklabels=y_test.columns)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()


# =========================================== 8. MODEL INTERPRETATION WITH SHAP ======================================
# ============================================== explainer: (DeepExplainer) ==========================================
# ====================================================================================================================

# Import SHAP library for model interpretation
import shap

# Create a SHAP explainer
# The SHAP explainer will explain the output of the neural network
# Sample a larger random subset from the training set as the background for a complex dataset
background = shap.utils.sample(X_train_scaled, 100)  # * we can adjust the number based on the complixity our dataset

# Create the SHAP explainer with the selected background dataset
explainer = shap.DeepExplainer(model, background)


# Compute SHAP values
# These values represent the impact of each feature on the model's prediction
shap_values = explainer.shap_values(X_test_scaled)



# ================================================     Plot SHAP values   ===========================================
# This plot will help understand how each feature influences the model's predictions

# Plot SHAP values for overall feature importance across all classes
# This plot will help understand the overall feature importance across all classes
shap.summary_plot(shap_values, X_test_scaled, plot_type='bar', feature_names=X.columns)


# ====================================================================================================================
# * If our dataset is very large or our model is complex, using more background samples 
# can be beneficial for a more accurate interpretation, but it will also demand more computational resources.
# - For a simple dataset: 100-300 samples
# - For complex datasets: 500-1000 or more 
# - starting with a smaller number of background samples: 100-500 samples


# ============================================ Interpretation of the above plot ======================================
# ====================================================================================================================
    # - This plot highlights which features the model finds most predictive for each class.
    # - By examining individual feature impacts, researchers can understand which genes are most critical in 
    # distinguishing among the response classes.
    # - The relatively smaller green bar segments across features suggest less differentiation for PD as compared to PR

# Strong Biomarker Genes:
    # 1- PR: 
        # - The large contributions of AC015689.1 and AC015468.3 to PR suggest they could be considered potential 
        # biomarkers indicative of a partial response.
        # - Genes like AC015689.1 and AC015468.3 stand out because their contributions to PR are highly significant, and 
        # they contribute less to SD or PD, making them more distinct as primary biomarkers for PR.

    # 2- SD: 
        # - MALAT1 and other features (e.g., ZFP36, MT-ND3, RPS29, and PABPC1) do show substantial contributions to both PR and SD classes.
        # - If a feature (gene) is contributing positively to more than one class but at different levels, 
        # it may not be the most reliable stand-alone biomarker for differentiating those classes.
        # - For instance, MALAT1 has more influence on PR than SD, making it less reliable for specifically identifying SD.
        # - MALAT1, ZFP36, and similar genes could serve as supporting features for SD predictions but might require 
        # additional complementary biomarkers to achieve higher specificity.


# ================================= My suggested strategies for Reliable Biomarker Identification ====================
# ====================================================================================================================
# 1- Combining Features:
        # - Panel Approach: 
        # - Use multiple features in combination to create a predictive panel. For example, while MALAT1 and ZFP36 alone 
        # might not differentiate well between PR and SD, combining their contributions with other distinct features could improve differentiation.


# 2-  Grouping genes into higher-order biological features: 
        # we can group genes into higher-order biological features like pathways, networks, or functional categories 
        # this allows models to capture underlying biological processes and relationships better.  
 
        # ============= Gene Grouping Based on: 
            # 1- Pathway-Based Transformation:
                # we can aggregate and group genes into pathways (e.g. KEGG, Reactome, and GO)
            
            # 2- Gene Set Enrichment Analysis (GSEA):
                # we can identify enriched pathways or gene sets

            # 3- Protein-Protein Interaction (PPI) Networks:
                # we can use PPI networks to group genes based on their protein interactions and identify significant modules 

            # 4- Gene Ontology (GO):
                # We can group features into functional categories using GO annotations.

            
            # ========== Advantages of this approach
            # 1- Reduction of Dimensionality

            # 2- Biological Relevance:
            # The model can focus on features that have known biological functions rather than isolated genes.
            
            # 3- Predictive Power:
            # This approach may highlight pathways or processes predictive of the clinical outcomes we are studying.

